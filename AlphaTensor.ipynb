{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=center> AlphaTensor </h1>"
      ],
      "metadata": {
        "id": "D8Oy84JS11nX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZxhwHjOvnWF"
      },
      "source": [
        "Loading factorizations found by AlphaTensor and recombination.\n",
        "\n",
        "- Copyright 2022 DeepMind Technologies Limited\n",
        "- All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0\n",
        "- All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).  You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "- Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.\n",
        "- This is not an official Google product."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithms"
      ],
      "metadata": {
        "id": "dzK4Ws5ujndT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv32k_zmYXta"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Required Files"
      ],
      "metadata": {
        "id": "i0N6N6aVg2X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-deepmind/alphatensor.git\n",
        "%cd alphatensor/algorithms"
      ],
      "metadata": {
        "id": "oPdm-ukGg7Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpWZeG2V3ZV0"
      },
      "source": [
        "Upload one of the two files provided in the same folder: `factorizations_r.npz` (algorithms in standard arithmetic) or `factorizations_f2.npz` (algorithms in arithmetic modulo 2)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Qmz4fQcwh6Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrXikWaPYO1n"
      },
      "outputs": [],
      "source": [
        "# Interactive upload\n",
        "# uploaded = files.upload()\n",
        "# filename = list(uploaded.keys())[0]\n",
        "# with open(filename, 'rb') as f:\n",
        "#   factorizations = dict(np.load(f, allow_pickle=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "FILE = \"factorizations_r.npz\" # @param [\"factorizations_r.npz\",\"factorizations_f2.npz\"] {\"allow-input\":true}\n",
        "with open(\"factorizations_r.npz\", 'rb') as f:\n",
        "  factorizations = dict(np.load(f, allow_pickle=True))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3LEgGLa2htkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factorizations.keys()"
      ],
      "metadata": {
        "id": "VZpIQKvSoM-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKiXSgk0YVRn"
      },
      "outputs": [],
      "source": [
        "# Print available factorizations and their shapes.\n",
        "for key in factorizations:\n",
        "  u, v, w = factorizations[key]\n",
        "  rank = u.shape[-1]\n",
        "  assert rank == v.shape[-1] and rank == w.shape[-1]\n",
        "  print(f'{key}: rank={u.shape[-1]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "factorizations[\"2,2,2\"]"
      ],
      "metadata": {
        "id": "PMXpfHipp5C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDbovsIXC8-H"
      },
      "source": [
        "Please note that as provided, the factorizations decompose the *symmetrized* version of the matrix multiplication tensor, representing the bilinear operation $\\mathbf{A}, \\mathbf{B} \\mapsto (\\mathbf{A} \\cdot \\mathbf{B})^T$. This is standard in the literature, and factorizations can be easily converted\n",
        "between the symmetrized and non-symmetrized versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZK9ReTpbGUu"
      },
      "outputs": [],
      "source": [
        "def get_mamu_tensor_rectangular(a: int, b: int, c: int) -> np.ndarray:\n",
        "  \"\"\"Returns the symmetrized matrix multiplication tensor T_{a, b, c}.\"\"\"\n",
        "  result = np.full((a*b, b*c, c*a), 0, dtype=np.int32)\n",
        "  for i in range(a):\n",
        "    for j in range(b):\n",
        "      for k in range(c):\n",
        "        result[i * b  + j][j * c + k][k * a + i] = 1\n",
        "  return result\n",
        "\n",
        "\n",
        "# Test correctness of a factorization.\n",
        "tensor = get_mamu_tensor_rectangular(3, 4, 5)\n",
        "u, v, w = factorizations['3,4,5']\n",
        "reconstruction = np.einsum('ir,jr,kr->ijk', u, v, w)\n",
        "if np.array_equal(tensor, reconstruction):\n",
        "  print('Factorization is correct in R (standard arithmetic).')\n",
        "elif np.array_equal(tensor, np.mod(reconstruction, 2)):\n",
        "  print('Factorization is correct in F2 (modular arithmetic).')\n",
        "else:\n",
        "  print('Factorization is incorrect.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"text-align: center; color: blue\"> Application </h2>"
      ],
      "metadata": {
        "id": "9_hINNVErrtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we actually get to apply the factorizations found by AlphaTensor to speed up multiplications of matrices we are interested in."
      ],
      "metadata": {
        "id": "MUYloBCTmMz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# The factor data for \"2,2,2\" as you provided\n",
        "factor_data = factorizations[\"2,2,2\"]\n",
        "\n",
        "# Dimensions for 2x2 multiplication\n",
        "m, k, n = 2, 2, 2\n",
        "R = 7  # Rank for 2x2x2 is 7\n",
        "\n",
        "# Extract U, V, W matrices (collections of factors)\n",
        "# U_all_ranks has shape (m*k, R) -> (4, 7)\n",
        "# V_all_ranks has shape (k*n, R) -> (4, 7)\n",
        "# W_all_ranks has shape (m*n, R) -> (4, 7)\n",
        "U_all_ranks = factor_data[0]\n",
        "V_all_ranks = factor_data[1]\n",
        "W_all_ranks = factor_data[2]\n",
        "\n",
        "# Let's define two toy 2x2 matrices A and B\n",
        "A = np.array([[1, 2],\n",
        "              [3, 4]], dtype=np.int32)\n",
        "\n",
        "B = np.array([[5, 6],\n",
        "              [7, 8]], dtype=np.int32)\n",
        "\n",
        "print(\"Matrix A:\\n\", A)\n",
        "print(\"Matrix B:\\n\", B)\n",
        "\n",
        "# --- AlphaTensor Multiplication ---\n",
        "\n",
        "# 1. Calculate M_r values (R of them)\n",
        "M_values = np.zeros(R, dtype=np.int32)\n",
        "for r in range(R):\n",
        "    u_r_flat = U_all_ranks[:, r]      # Shape (4,)\n",
        "    u_r_matrix = u_r_flat.reshape(m, k) # Shape (2, 2)\n",
        "    M_values[r] = np.sum(u_r_matrix * A)\n",
        "\n",
        "# 2. Calculate N_r values (R of them)\n",
        "N_values = np.zeros(R, dtype=np.int32)\n",
        "for r in range(R):\n",
        "    v_r_flat = V_all_ranks[:, r]      # Shape (4,)\n",
        "    v_r_matrix = v_r_flat.reshape(k, n) # Shape (2, 2)\n",
        "    N_values[r] = np.sum(v_r_matrix * B)\n",
        "\n",
        "# 3. Calculate P_r = M_r * N_r (the R scalar multiplications)\n",
        "P_values = M_values * N_values\n",
        "\n",
        "# 4. Reconstruct C_alpha\n",
        "C_alpha = np.zeros((m, n), dtype=np.int32)\n",
        "for r in range(R):\n",
        "    w_r_flat = W_all_ranks[:, r]      # Shape (4,)\n",
        "\n",
        "    # Assumes row-major order (C order), which is NumPy's default.\n",
        "    # Results in a transposed C Matrix\n",
        "    # w_r_matrix = w_r_flat.reshape(m, n) # Shape (2, 2)\n",
        "\n",
        "    # use Fortran order when reshaping the W factor's flat data.\n",
        "    # to avoid getting a transpose of our intended C Matrix\n",
        "    w_r_matrix = W_all_ranks[:, r].reshape((m, n), order='F')\n",
        "    C_alpha += P_values[r] * w_r_matrix\n",
        "\n",
        "print(\"\\nResult C_alpha (using AlphaTensor factors):\\n\", C_alpha)\n",
        "\n",
        "# --- Standard Matrix Multiplication for comparison ---\n",
        "C_standard = A @ B\n",
        "# Or using np.matmul(A, B)\n",
        "\n",
        "print(\"\\nResult C_standard (using A @ B):\\n\", C_standard)\n",
        "\n",
        "# Check if the results are the same\n",
        "if np.array_equal(C_alpha, C_standard):\n",
        "    print(\"\\nSuccess! AlphaTensor's method matches standard multiplication.\")\n",
        "else:\n",
        "    print(\"\\nError! Results do not match.\")\n",
        "    print(\"Difference:\", C_alpha - C_standard)"
      ],
      "metadata": {
        "id": "2bekb_Xastks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"text-align: center\"> General Fast Multiplication Algorithm </h3>\n",
        "\n",
        "This is the holy grail we have been searching for all this time, **a general way to fast multiply arbitrarily sized matrices**"
      ],
      "metadata": {
        "id": "L76BTxNOyEmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title General Fast Multiplication Algorithm\n",
        "def multiply_with_alphatensor_factors(A, B, key, factorizations):\n",
        "    \"\"\"\n",
        "    Multiplies matrices A and B using AlphaTensor factors for the given key.\n",
        "\n",
        "    Args:\n",
        "        A (np.ndarray): The first input matrix.\n",
        "        B (np.ndarray): The second input matrix.\n",
        "        key (str): The key corresponding to the factorization (e.g., \"m,k,n\").\n",
        "        factorizations (dict): A dictionary where keys are \"m,k,n\" strings\n",
        "                               and values are the (3, size, R) factor tensors.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The resulting matrix C = A @ B.\n",
        "        Or None if dimensions are mismatched or key is not found.\n",
        "    \"\"\"\n",
        "    if key not in factorizations:\n",
        "        print(f\"Error: Factorization key '{key}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # 1. Parse m, k, n from the key\n",
        "    try:\n",
        "        m_str, k_str, n_str = key.split(',')\n",
        "        m, k, n = int(m_str), int(k_str), int(n_str)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Key '{key}' is not in the format 'm,k,n'.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Validate input matrix dimensions\n",
        "    if A.shape != (m, k):\n",
        "        print(f\"Error: Matrix A shape {A.shape} does not match key dimensions ({m},{k}).\")\n",
        "        return None\n",
        "    if B.shape != (k, n):\n",
        "        print(f\"Error: Matrix B shape {B.shape} does not match key dimensions ({k},{n}).\")\n",
        "        return None\n",
        "\n",
        "    # 3. Extract factor data and Rank (R)\n",
        "    factor_data = factorizations[key] # This is the (3, size_flat, R) array\n",
        "    # U_all_ranks has shape (m*k, R)\n",
        "    # V_all_ranks has shape (k*n, R)\n",
        "    # W_all_ranks has shape (m*n, R)\n",
        "    U_all_ranks = factor_data[0]\n",
        "    V_all_ranks = factor_data[1]\n",
        "    W_all_ranks = factor_data[2]\n",
        "\n",
        "    # The rank R is the number of columns in U_all_ranks (or V or W)\n",
        "    if U_all_ranks.shape[0] != m * k:\n",
        "        print(f\"Error: U factor dimension mismatch. Expected {m*k}, got {U_all_ranks.shape[0]}\")\n",
        "        return None\n",
        "    if V_all_ranks.shape[0] != k * n:\n",
        "        print(f\"Error: V factor dimension mismatch. Expected {k*n}, got {V_all_ranks.shape[0]}\")\n",
        "        return None\n",
        "    if W_all_ranks.shape[0] != m * n:\n",
        "        print(f\"Error: W factor dimension mismatch. Expected {m*n}, got {W_all_ranks.shape[0]}\")\n",
        "        return None\n",
        "\n",
        "    R = U_all_ranks.shape[1]\n",
        "    if V_all_ranks.shape[1] != R or W_all_ranks.shape[1] != R:\n",
        "        print(f\"Error: Rank mismatch between U, V, W factors for key '{key}'.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Using AlphaTensor factors for {m}x{k} @ {k}x{n} multiplication with Rank R={R}\")\n",
        "\n",
        "    # --- AlphaTensor Multiplication Logic ---\n",
        "    A_flat = A.flatten() # Should be row-major, which is NumPy's default\n",
        "    B_flat = B.flatten()\n",
        "\n",
        "    # 1. Calculate M_r values\n",
        "    # M_values[r] = sum_{i,j} U_all_ranks[idx(i,j), r] * A[i,j]\n",
        "    # This can be done efficiently with dot products if A is flattened\n",
        "    # M_values[r] = U_all_ranks[:, r].reshape(m, k) * A  then sum over all elements\n",
        "    # OR: M_values[r] = np.dot(A_flat, U_all_ranks[:,r]) if U factors were stored as (R, m*k)\n",
        "    # Given U_all_ranks is (m*k, R), we sum (U_r_matrix * A)\n",
        "    M_values = np.zeros(R, dtype=A.dtype) # Match dtype of input\n",
        "    for r_idx in range(R):\n",
        "        u_r_matrix = U_all_ranks[:, r_idx].reshape(m, k)\n",
        "        M_values[r_idx] = np.sum(u_r_matrix * A)\n",
        "\n",
        "\n",
        "    # 2. Calculate N_r values\n",
        "    N_values = np.zeros(R, dtype=B.dtype)\n",
        "    for r_idx in range(R):\n",
        "        v_r_matrix = V_all_ranks[:, r_idx].reshape(k, n)\n",
        "        N_values[r_idx] = np.sum(v_r_matrix * B)\n",
        "\n",
        "    # 3. Calculate P_r = M_r * N_r\n",
        "    P_values = M_values * N_values\n",
        "\n",
        "    # 4. Reconstruct C_alpha\n",
        "    C_alpha = np.zeros((m, n), dtype=P_values.dtype)\n",
        "    for r_idx in range(R):\n",
        "        # Use order='F' for W as previously determined!\n",
        "        w_r_matrix = W_all_ranks[:, r_idx].reshape((m, n), order='F')\n",
        "        C_alpha += P_values[r_idx] * w_r_matrix\n",
        "\n",
        "    return C_alpha"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hB6znTJvyB6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_2 = np.array([[1, 2, 3],\n",
        "                [4, 5, 6],\n",
        "                [7, 8, 9]])\n",
        "B_2 = np.array([[1, 2, 3],\n",
        "                [4, 5, 6],\n",
        "                [7, 8, 9]])\n",
        "\n",
        "multiply_with_alphatensor_factors(A_2, B_2, \"3,3,3\", factorizations)"
      ],
      "metadata": {
        "id": "Zpm0IZT4ytbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"text-align: center; color: green\"> Future Work </h3>\n",
        "\n",
        "1. Compute the factorization dictionary key from the shape of the input matrices.\n",
        "2. How is this to be implemented in a lower level language (C, C++)\n",
        "3. How is this to be implemented on the GPU.  \n",
        "- CUDA C++\n",
        "- Parallelization\n",
        "4. Can this be implemented on a TPU?\n",
        "5. How do we error handle for incompatible matrix shapes?"
      ],
      "metadata": {
        "id": "zSZJZ5mb0yD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recombination\n",
        "\n",
        "This expands on the basic cases that AlphaTensor found, specifically for larger matrix sizes. It essentially\n",
        "> provides tools to construct new U, V, W factorizations for larger matrix sizes by combining known factorizations for smaller ones."
      ],
      "metadata": {
        "id": "Pgk7vkczjtDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "ixb6N7SpjzZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75ozbYv50Aeg"
      },
      "outputs": [],
      "source": [
        "!python3 -m alphatensor.recombination.example"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxcEMsZ5j24W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}